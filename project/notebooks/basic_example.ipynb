{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eee5bee-61e5-4918-9c47-fe26eab6cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for binary classification\n",
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6afa0b-83c8-4f84-b096-46478513ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cdc610-c2ab-41e0-af48-9c3933142d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66d58ab0-416e-4dfa-8790-39c45e3cb084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 116\n",
      "0.7205\n",
      "0.5385\n",
      "0.6774\n",
      "0.5912\n",
      "0.7828\n",
      "0.5626\n",
      "0.7729\n",
      "0.7102\n",
      "0.5953\n",
      "0.5964\n",
      "0.5356\n",
      "0.6200\n",
      "0.5374\n",
      "0.7515\n",
      "0.6818\n",
      "0.6154\n",
      "0.5947\n",
      "0.5396\n",
      "0.6067\n",
      "0.5839\n",
      "0.6089\n",
      "0.6478\n",
      "0.5100\n",
      "0.5573\n",
      "0.5718\n",
      "0.6046\n",
      "0.5684\n",
      "0.5938\n",
      "0.5130\n",
      "0.4868\n",
      "0.5735\n",
      "0.5225\n",
      "0.6298\n",
      "0.5563\n",
      "0.5997\n",
      "0.4898\n",
      "0.4732\n",
      "0.4715\n",
      "0.5105\n",
      "0.4496\n",
      "0.6660\n",
      "0.4166\n",
      "0.4589\n",
      "0.4794\n",
      "0.5310\n",
      "0.3691\n",
      "0.6483\n",
      "0.4443\n",
      "0.4801\n",
      "0.5184\n",
      "0.5186\n",
      "0.5269\n",
      "0.4448\n",
      "0.4465\n",
      "0.4829\n",
      "0.4367\n",
      "0.4880\n",
      "0.3616\n",
      "0.4778\n",
      "0.5347\n",
      "0.4358\n",
      "0.3636\n",
      "0.5694\n",
      "0.4803\n",
      "0.5096\n",
      "0.4310\n",
      "0.4142\n",
      "0.4322\n",
      "0.4949\n",
      "0.4341\n",
      "0.4020\n",
      "0.2767\n",
      "0.3994\n",
      "0.4784\n",
      "0.3342\n",
      "0.4295\n",
      "0.4432\n",
      "0.3950\n",
      "0.4948\n",
      "0.2739\n",
      "0.3361\n",
      "0.4689\n",
      "0.4310\n",
      "0.4249\n",
      "0.2971\n",
      "0.4553\n",
      "0.3524\n",
      "0.4395\n",
      "0.3528\n",
      "0.3426\n",
      "0.3332\n",
      "0.4077\n",
      "0.4936\n",
      "0.2479\n",
      "0.4557\n",
      "0.5371\n",
      "0.3817\n",
      "0.3153\n",
      "0.4543\n",
      "0.3216\n",
      "0.4163\n",
      "0.3182\n",
      "0.3529\n",
      "0.3382\n",
      "0.3381\n",
      "0.3971\n",
      "0.3942\n",
      "0.3021\n",
      "0.2276\n",
      "0.4821\n",
      "0.3494\n",
      "0.1986\n",
      "0.3062\n",
      "0.3895\n",
      "0.3682\n",
      "0.3132\n",
      "0.2132\n",
      "0.3637\n",
      "0.3175\n",
      "0.4989\n",
      "0.2564\n",
      "0.3882\n",
      "0.2752\n",
      "0.4063\n",
      "0.3065\n",
      "0.2787\n",
      "0.3541\n",
      "0.1889\n",
      "0.2533\n",
      "0.3001\n",
      "0.2810\n",
      "0.3168\n",
      "0.3232\n",
      "0.3109\n",
      "0.3237\n",
      "0.3911\n",
      "0.2835\n",
      "0.3352\n",
      "0.2833\n",
      "0.3089\n",
      "0.2506\n",
      "0.2545\n",
      "0.3195\n",
      "0.2553\n",
      "0.2885\n",
      "0.2244\n",
      "0.3327\n",
      "0.1774\n",
      "0.3607\n",
      "0.3352\n",
      "0.2501\n",
      "0.2458\n",
      "0.2964\n",
      "0.1606\n",
      "0.2468\n",
      "0.2366\n",
      "0.2194\n",
      "0.3474\n",
      "0.3514\n",
      "0.3762\n",
      "0.1837\n",
      "0.1651\n",
      "0.4240\n",
      "0.2818\n",
      "0.1618\n",
      "0.1954\n",
      "0.4155\n",
      "0.2132\n",
      "0.2459\n",
      "0.2725\n",
      "0.2231\n",
      "0.2656\n",
      "0.3412\n",
      "0.2160\n",
      "0.2153\n",
      "0.1752\n",
      "0.1210\n",
      "0.3143\n",
      "0.2235\n",
      "0.2568\n",
      "0.2513\n",
      "0.1344\n",
      "0.3044\n",
      "0.5073\n",
      "0.2101\n",
      "0.2605\n",
      "0.2854\n",
      "0.1569\n",
      "0.3272\n",
      "0.2019\n",
      "0.2380\n",
      "0.0905\n",
      "0.2552\n",
      "0.2220\n",
      "0.1506\n",
      "0.2758\n",
      "0.2539\n",
      "0.1732\n",
      "0.2292\n",
      "0.3156\n",
      "0.1964\n",
      "0.1520\n",
      "0.2391\n",
      "0.3385\n",
      "0.2650\n",
      "0.1544\n",
      "0.2193\n",
      "0.1217\n",
      "0.2120\n",
      "0.2797\n",
      "0.2299\n",
      "0.2147\n",
      "0.1495\n",
      "0.1379\n",
      "0.3195\n",
      "0.0418\n",
      "0.1632\n",
      "0.3064\n",
      "0.1968\n",
      "0.1255\n",
      "0.2377\n",
      "0.2823\n",
      "0.1639\n",
      "0.1033\n",
      "0.1998\n",
      "0.2353\n",
      "0.1269\n",
      "0.1931\n",
      "0.1951\n",
      "0.2108\n",
      "0.2522\n",
      "0.1372\n",
      "0.2666\n",
      "0.1277\n",
      "0.2274\n",
      "0.2111\n",
      "0.1608\n",
      "0.0747\n",
      "0.2030\n",
      "0.4617\n",
      "0.1403\n",
      "0.1938\n",
      "0.2034\n",
      "0.2824\n",
      "0.1160\n",
      "0.2177\n",
      "0.1640\n",
      "0.1817\n",
      "0.1913\n",
      "0.1669\n",
      "0.1925\n",
      "0.1630\n",
      "0.1719\n",
      "0.1559\n",
      "0.2273\n",
      "0.2020\n",
      "0.2084\n",
      "0.2048\n",
      "0.2257\n",
      "0.1422\n",
      "0.1153\n",
      "0.1279\n",
      "0.2453\n",
      "0.0904\n",
      "0.2357\n",
      "0.1641\n",
      "0.1585\n",
      "0.1306\n",
      "0.3165\n",
      "0.1306\n",
      "0.0989\n",
      "0.0638\n",
      "0.1731\n",
      "0.2497\n",
      "0.1946\n",
      "0.2152\n",
      "0.1761\n",
      "0.0829\n",
      "0.1080\n",
      "0.0827\n",
      "0.1187\n",
      "0.1424\n",
      "0.1918\n",
      "0.1307\n",
      "0.2476\n",
      "0.1498\n",
      "0.1159\n",
      "0.2894\n",
      "0.2059\n",
      "0.1784\n",
      "0.1365\n",
      "0.1978\n",
      "0.1702\n",
      "0.1297\n",
      "0.0945\n",
      "0.1313\n",
      "0.1340\n",
      "0.1041\n",
      "0.1413\n",
      "0.2792\n",
      "0.0930\n",
      "0.2432\n",
      "0.0870\n",
      "0.1192\n",
      "0.1237\n",
      "0.1841\n",
      "0.0919\n",
      "0.1865\n",
      "0.1272\n",
      "0.2095\n",
      "0.0847\n",
      "0.2361\n",
      "0.1592\n",
      "0.1026\n",
      "0.1456\n",
      "0.0869\n",
      "0.1352\n",
      "0.1264\n",
      "0.2931\n",
      "0.0415\n",
      "0.0862\n",
      "0.1837\n",
      "0.0493\n",
      "0.1321\n",
      "0.1548\n",
      "0.2005\n",
      "0.1778\n",
      "0.1291\n",
      "0.1156\n",
      "0.0993\n",
      "0.0961\n",
      "0.1215\n",
      "0.1237\n",
      "0.2616\n",
      "0.1705\n",
      "0.0401\n",
      "0.1829\n",
      "0.0695\n",
      "0.2034\n",
      "0.1261\n",
      "0.1009\n",
      "0.1594\n",
      "0.0462\n",
      "0.2847\n",
      "0.1859\n",
      "0.1117\n",
      "0.1083\n",
      "0.0663\n",
      "0.1321\n",
      "0.1119\n",
      "0.2022\n",
      "0.0941\n",
      "0.0906\n",
      "0.1207\n",
      "0.2545\n",
      "0.1239\n",
      "0.0856\n",
      "0.0812\n",
      "0.1274\n",
      "0.1505\n",
      "0.1026\n",
      "0.0804\n",
      "0.1080\n",
      "0.1767\n",
      "0.1451\n",
      "0.1359\n",
      "0.1287\n",
      "0.0782\n",
      "0.1425\n",
      "0.1889\n",
      "0.0814\n",
      "0.0759\n",
      "0.1253\n",
      "0.1701\n",
      "0.0708\n",
      "0.1059\n",
      "0.1674\n",
      "0.1130\n",
      "0.1152\n",
      "0.1449\n",
      "0.0673\n",
      "0.1269\n",
      "0.1154\n",
      "0.0253\n",
      "0.1188\n",
      "0.1147\n",
      "0.2320\n",
      "0.0744\n",
      "0.0620\n",
      "0.0602\n",
      "0.1543\n",
      "0.0655\n",
      "0.1506\n",
      "0.1563\n",
      "0.1120\n",
      "0.1293\n",
      "0.0778\n",
      "0.1034\n",
      "0.0729\n",
      "0.0609\n",
      "0.0818\n",
      "0.0537\n",
      "0.1995\n",
      "0.2112\n",
      "0.0555\n",
      "0.1099\n",
      "0.0774\n",
      "0.0322\n",
      "0.1571\n",
      "0.1238\n",
      "0.0566\n",
      "0.0656\n",
      "0.0751\n",
      "0.1068\n",
      "0.1194\n",
      "0.2358\n",
      "0.2309\n",
      "0.0983\n",
      "0.0511\n",
      "0.0751\n",
      "0.0783\n",
      "0.1169\n",
      "0.0705\n",
      "0.1533\n",
      "0.0888\n",
      "0.1371\n",
      "0.0510\n",
      "0.0652\n",
      "0.1569\n",
      "0.1691\n",
      "0.0651\n",
      "0.0253\n",
      "0.0502\n",
      "0.1836\n",
      "0.0440\n",
      "0.0607\n",
      "0.1054\n",
      "0.1912\n",
      "0.0867\n",
      "0.0403\n",
      "0.0612\n",
      "0.1100\n",
      "0.1543\n",
      "0.1641\n",
      "0.0752\n",
      "0.0608\n",
      "0.0851\n",
      "0.0359\n",
      "0.0920\n",
      "0.0599\n",
      "0.2370\n",
      "0.0663\n",
      "0.0736\n",
      "0.0922\n",
      "0.0696\n",
      "0.0389\n",
      "0.0827\n",
      "0.1737\n",
      "0.0528\n",
      "0.0861\n",
      "0.1157\n",
      "0.1105\n",
      "0.0375\n",
      "0.0906\n",
      "0.0512\n",
      "0.0331\n",
      "0.1327\n",
      "0.1650\n",
      "0.1148\n",
      "0.1029\n",
      "0.0520\n",
      "0.0731\n",
      "0.1293\n",
      "0.0412\n",
      "0.1042\n",
      "0.0565\n",
      "0.0863\n",
      "0.0641\n",
      "0.1793\n",
      "0.0157\n",
      "0.0637\n",
      "0.0363\n",
      "0.1226\n",
      "0.0863\n",
      "0.1275\n",
      "0.1022\n",
      "0.0998\n",
      "0.0377\n",
      "0.1222\n",
      "0.0547\n",
      "0.0652\n",
      "0.1474\n",
      "0.0554\n",
      "0.1054\n",
      "0.0651\n",
      "0.0545\n",
      "0.0475\n",
      "0.0648\n",
      "0.0409\n",
      "0.0240\n",
      "0.0393\n",
      "0.2944\n",
      "0.1320\n",
      "0.0197\n",
      "0.0477\n",
      "0.0909\n",
      "0.0536\n",
      "0.0641\n",
      "0.0962\n",
      "0.0802\n",
      "0.1613\n",
      "0.0484\n",
      "0.0397\n",
      "0.2080\n",
      "0.0577\n",
      "0.0424\n",
      "0.1324\n",
      "0.0432\n",
      "0.0693\n",
      "0.0499\n",
      "0.0325\n",
      "0.0628\n",
      "0.1005\n",
      "0.0378\n",
      "0.2428\n",
      "0.0495\n",
      "0.0538\n",
      "0.0370\n",
      "0.1758\n",
      "0.0420\n",
      "0.0542\n",
      "0.0273\n",
      "0.0496\n",
      "0.1084\n",
      "0.0974\n",
      "0.0810\n",
      "0.0625\n",
      "0.0268\n",
      "0.0356\n",
      "0.0463\n",
      "0.0800\n",
      "0.0392\n",
      "0.2177\n",
      "0.2100\n",
      "0.1076\n",
      "0.0253\n",
      "0.0683\n",
      "0.1772\n",
      "0.0734\n",
      "0.0619\n",
      "0.0613\n",
      "0.0445\n",
      "0.0410\n",
      "0.0915\n",
      "0.0726\n",
      "0.0371\n",
      "0.1153\n",
      "0.0817\n",
      "0.0953\n",
      "0.0419\n",
      "0.1252\n",
      "0.0457\n",
      "0.0305\n",
      "0.1880\n",
      "0.0299\n",
      "0.0806\n",
      "0.0362\n",
      "0.0337\n",
      "0.0533\n",
      "0.1271\n",
      "0.1262\n",
      "0.0509\n",
      "0.0398\n",
      "0.0148\n",
      "0.1046\n",
      "0.0489\n",
      "0.1368\n",
      "0.0381\n",
      "0.0749\n",
      "0.0875\n",
      "0.0456\n",
      "0.0758\n",
      "0.0541\n",
      "0.0602\n",
      "0.0632\n",
      "0.0997\n",
      "0.0228\n",
      "0.0656\n",
      "0.1181\n",
      "0.1049\n",
      "0.0474\n",
      "0.0659\n",
      "0.1551\n",
      "0.0540\n",
      "0.0221\n",
      "0.0537\n",
      "0.0435\n",
      "0.1458\n",
      "0.0191\n",
      "0.0511\n",
      "0.0329\n",
      "0.0525\n",
      "0.1241\n",
      "0.1108\n",
      "0.0238\n",
      "0.0425\n",
      "0.0841\n",
      "0.0782\n",
      "0.0338\n",
      "0.0890\n",
      "0.0184\n",
      "0.0692\n",
      "0.0321\n",
      "0.0350\n",
      "0.1541\n",
      "0.1757\n",
      "0.0163\n",
      "0.0893\n",
      "0.0578\n",
      "0.0328\n",
      "0.1788\n",
      "0.0389\n",
      "0.0557\n",
      "0.0638\n",
      "0.0925\n",
      "0.0436\n",
      "0.0692\n",
      "0.0244\n",
      "0.0591\n",
      "0.0257\n",
      "0.0792\n",
      "0.2559\n",
      "0.0302\n",
      "0.0547\n",
      "0.0854\n",
      "0.1167\n",
      "0.0501\n",
      "0.0541\n",
      "0.0655\n",
      "0.0685\n",
      "0.0490\n",
      "0.0191\n",
      "0.0251\n",
      "0.0495\n",
      "0.0664\n",
      "0.1611\n",
      "0.1160\n",
      "0.0131\n",
      "0.0442\n",
      "0.1014\n",
      "0.0193\n",
      "0.0248\n",
      "0.0260\n",
      "0.0385\n",
      "0.1164\n",
      "0.2759\n",
      "0.0296\n",
      "0.0734\n",
      "0.0328\n",
      "0.1779\n",
      "0.0575\n",
      "0.0508\n",
      "0.0461\n",
      "0.0157\n",
      "0.0430\n",
      "0.0883\n",
      "0.0522\n",
      "0.0461\n",
      "0.0369\n",
      "0.1133\n",
      "0.0340\n",
      "0.1647\n",
      "0.0311\n",
      "0.0878\n",
      "0.1382\n",
      "0.0285\n",
      "0.0265\n",
      "0.0571\n",
      "0.0693\n",
      "0.0193\n",
      "0.0343\n",
      "0.0371\n",
      "0.0798\n",
      "0.0673\n",
      "0.0243\n",
      "0.1425\n",
      "0.0308\n",
      "0.0622\n",
      "0.0570\n",
      "0.1573\n",
      "0.0472\n",
      "0.0346\n",
      "0.0291\n",
      "0.0457\n",
      "0.0251\n",
      "0.1210\n",
      "0.0181\n",
      "0.0302\n",
      "0.0663\n",
      "0.0576\n",
      "0.0751\n",
      "0.1066\n",
      "0.0517\n",
      "0.0507\n",
      "0.0220\n",
      "0.1045\n",
      "0.0837\n",
      "0.0767\n",
      "0.0572\n",
      "0.0240\n",
      "0.0552\n",
      "0.0178\n",
      "0.0308\n",
      "0.0249\n",
      "0.0360\n",
      "0.0335\n",
      "0.0335\n",
      "0.2085\n",
      "0.0425\n",
      "0.0183\n",
      "0.0311\n",
      "0.1394\n",
      "0.0369\n",
      "0.0792\n",
      "0.0595\n",
      "0.0229\n",
      "0.0310\n",
      "0.0456\n",
      "0.0830\n",
      "0.0205\n",
      "0.0321\n",
      "0.1292\n",
      "0.0387\n",
      "0.0332\n",
      "0.0550\n",
      "0.0394\n",
      "0.0558\n",
      "0.0291\n",
      "0.0586\n",
      "0.1020\n",
      "0.0517\n",
      "0.0862\n",
      "0.0089\n",
      "0.0259\n",
      "0.0466\n",
      "0.0320\n",
      "0.0204\n",
      "0.0295\n",
      "0.1065\n",
      "0.0855\n",
      "0.0525\n",
      "0.0559\n",
      "0.0411\n",
      "0.0532\n",
      "0.0124\n",
      "0.0488\n",
      "0.0265\n",
      "0.0873\n",
      "0.1223\n",
      "0.0095\n",
      "0.0279\n",
      "0.0555\n",
      "0.0404\n",
      "0.0495\n",
      "0.0190\n",
      "0.1513\n",
      "0.0199\n",
      "0.0338\n",
      "0.0537\n",
      "0.0259\n",
      "0.0501\n",
      "0.0435\n",
      "0.0392\n",
      "0.0436\n",
      "0.0248\n",
      "0.2841\n",
      "0.0197\n",
      "0.0400\n",
      "0.1136\n",
      "0.0381\n",
      "0.0577\n",
      "0.0419\n",
      "0.0785\n",
      "0.0529\n",
      "0.0458\n",
      "0.0239\n",
      "0.0405\n",
      "0.0380\n",
      "0.0329\n",
      "0.0539\n",
      "0.1740\n",
      "0.0162\n",
      "0.0155\n",
      "0.0247\n",
      "0.0650\n",
      "0.0523\n",
      "0.0513\n",
      "0.0268\n",
      "0.0322\n",
      "0.2585\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    " \n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"{loss.item():.4f}\")\n",
    " \n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    " \n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    " \n",
    "# prepare the data\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "model = MLP(34)\n",
    "# train the model\n",
    "train_model(train_dl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c814579a-6725-49dd-b626-608293e5632e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.922\n",
      "Predicted: 0.999 (class=1)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction (expect class=1)\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee828f1-9230-48cd-ac22-f324add2bc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed132b2d-31c1-420e-9cd0-040743a88e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch",
   "language": "python",
   "name": "venv_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
